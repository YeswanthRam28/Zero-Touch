<div align="center">

# üß† Zero-Touch ‚Äì Multimodal Surgeon Assistant

![Zero-Touch](https://img.shields.io/badge/Zero--Touch-Surgical%20Intelligence-blueviolet?style=for-the-badge&logo=medical-cross&logoColor=white)

**üè• Gesture + Gaze + Voice Controlled Surgical Assistant**

[![Privacy First](https://img.shields.io/badge/Privacy-Local%20First-green?style=flat-square)](#)
[![Multimodal](https://img.shields.io/badge/Multimodal-Gesture%20%2B%20Gaze%20%2B%20Voice-orange?style=flat-square)](#)
[![Hybrid AI](https://img.shields.io/badge/Hybrid-AI%20Cloud%20%2B%20Local-blue?style=flat-square)](#)
[![Real-Time](https://img.shields.io/badge/Real--Time-Low%20Latency-brightgreen?style=flat-square)](#)

*Enabling hands-free surgical image navigation through multimodal AI fusion*

</div>

---

## üéØ Problem Statement

Surgeons need to interact with medical imaging **without breaking sterility or workflow**.

### üîç The Challenge

* Surgeons cannot touch screens or keyboards during sterile procedures
* Traditional voice-only systems lack precision for medical imaging
* Existing solutions don't combine gesture, gaze, and voice intelligently
* High latency and cloud dependency compromise real-time surgical workflows
* Lack of context-aware multimodal fusion reduces accuracy

---

## üöÄ Our Solution: Zero-Touch Multimodal Surgeon Assistant

Zero-Touch is a **real-time, multimodal surgical assistant** that fuses **gesture tracking, gaze estimation, and voice commands** to enable hands-free, sterile interaction with medical imaging systems.

It combines **MediaPipe vision models**, **Whisper ASR**, and **lightweight LLMs** with a custom **multimodal fusion engine** to achieve precise, context-aware surgical navigation.

<div align="center">

```mermaid
graph TD
    A[üëã Gesture Input] --> D[Multimodal Fusion Engine]
    B[üëÅÔ∏è Gaze Tracking] --> D
    C[üéôÔ∏è Voice Commands] --> D
    
    A --> A1[MediaPipe Hands]
    B --> B1[MediaPipe Face Mesh]
    C --> C1[Whisper ASR]
    
    A1 --> A2[Pinch/Swipe/Wave Detection]
    B1 --> B2[Eye Landmark Extraction]
    C1 --> C2[Command Parser LLM]
    
    A2 --> D
    B2 --> D
    C2 --> D
    
    D --> E[Timestamp Alignment]
    E --> F[Intent Classifier]
    F --> G[Action Dispatcher]
    
    G --> H1[Zoom Region]
    G --> H2[Scroll/Pan]
    G --> H3[Highlight Area]
    G --> H4[Load Image]
    
    H1 --> I[Visual Feedback Overlay]
    H2 --> I
    H3 --> I
    H4 --> I
```

</div>

---

## ‚≠ê Key Features

### ÔøΩ Real-Time Gesture Tracking

* **MediaPipe Hands** for precise hand landmark detection
* Pinch, swipe, wave, and custom gesture recognition
* Region-of-interest selection via hand pointing
* Adaptive gesture personalization per surgeon

---

### üëÅÔ∏è Gaze Estimation & Screen Mapping

* **MediaPipe Face Mesh** for eye landmark extraction
* Real-time gaze direction estimation
* Calibrated screen coordinate mapping
* Visual feedback overlay showing gaze focus
* Tested under surgical lighting conditions

---

### üéôÔ∏è Voice Command Intelligence

* **Whisper Tiny** for local, low-latency ASR
* Lightweight LLM (TinyLlama/Phi-2) for flexible command parsing
* Error recovery with clarification prompts
* Memory state tracking (active image, mode, context)
* Medical terminology support

---

### üß† Multimodal Fusion Engine

* Timestamp-aligned fusion of gesture + gaze + voice
* Rule-based and transformer-based intent classification
* Context-aware action disambiguation
* Confidence scoring for each modality
* Fallback strategies for low-confidence inputs

---

### ‚ö° Low-Latency Real-Time Processing

* **<200ms** end-to-end latency target
* Local-first processing (no cloud dependency)
* Optimized inference pipelines
* Parallel modality processing
* Hardware acceleration support

---

### üéØ Surgical Workflow Integration

* Sterile, hands-free operation
* Medical image navigation (zoom, pan, scroll)
* Region highlighting and annotation
* Multi-image comparison support
* Customizable action mappings

---

## üèóÔ∏è System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Gesture Stream ‚îÇ  ‚îÇ   Gaze Stream   ‚îÇ  ‚îÇ   Voice Stream  ‚îÇ
‚îÇ  MediaPipe Hands‚îÇ  ‚îÇ MediaPipe Face  ‚îÇ  ‚îÇ  Whisper ASR    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                    ‚îÇ                    ‚îÇ
         ‚ñº                    ‚ñº                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Gesture Parser  ‚îÇ  ‚îÇ  Gaze Calibrator‚îÇ  ‚îÇ Command Parser  ‚îÇ
‚îÇ (Pinch/Swipe)   ‚îÇ  ‚îÇ (Screen Coords) ‚îÇ  ‚îÇ  (LLM/Rules)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                    ‚îÇ                    ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Fusion Engine     ‚îÇ
                    ‚îÇ  Timestamp Align   ‚îÇ
                    ‚îÇ  Intent Classifier ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Action Dispatcher ‚îÇ
                    ‚îÇ  State Manager     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚ñº                    ‚ñº                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Image Control  ‚îÇ  ‚îÇ Visual Feedback ‚îÇ  ‚îÇ Audio Feedback  ‚îÇ
‚îÇ  (Zoom/Pan)     ‚îÇ  ‚îÇ   Overlay       ‚îÇ  ‚îÇ   (TTS)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üõ†Ô∏è Technology Stack

<div align="center">

### Core

![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-Vision-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-ML%20Framework-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)

### Vision & Gesture

![MediaPipe](https://img.shields.io/badge/MediaPipe-Hands%20%2B%20Face-00C853?style=for-the-badge)
![Gaze Tracking](https://img.shields.io/badge/Gaze-Estimation-4285F4?style=for-the-badge)

### Voice & Language

![Whisper](https://img.shields.io/badge/Whisper-Tiny%20ASR-412991?style=for-the-badge)
![TinyLlama](https://img.shields.io/badge/TinyLlama-Command%20Parser-FF6B6B?style=for-the-badge)
![Phi-2](https://img.shields.io/badge/Phi--2-Lightweight%20LLM-00A4EF?style=for-the-badge)

### Fusion & Integration

![Transformer](https://img.shields.io/badge/Transformer-Multimodal%20Fusion-FFA500?style=for-the-badge)
![FastAPI](https://img.shields.io/badge/FastAPI-Backend-009688?style=for-the-badge&logo=fastapi&logoColor=white)

</div>

---

## üîê Ethics & Safety

* Privacy-first architecture
* User-controlled data
* Non-prescriptive AI responses
* Transparent AI decisions
* Accessibility-focused design

---

## üöÄ Getting Started

### Prerequisites

```bash
Python 3.8+
pip
Ollama (optional, for local AI)
Webcam/Camera device
```

### Installation

```bash
git clone https://github.com/YeswanthRam28/Zero-Touch.git
cd Zero-Touch
pip install -r requirements.txt
python main_audio.py
```

---

## üì∏ Project Components

## üéØ Use Cases

* **Sterile Surgical Procedures**: Navigate medical imaging without breaking sterility
* **Operating Room Workflows**: Hands-free control during active surgery
* **Medical Image Review**: Zoom, pan, and annotate diagnostic images
* **Radiology Consultations**: Multi-image comparison and analysis
* **Training & Simulation**: Surgical education with multimodal interaction
* **Emergency Medicine**: Rapid image access in time-critical situations

---

## üèÜ Innovation Highlights

* **Multimodal Fusion**: First-of-its-kind gesture + gaze + voice integration for surgery
* **Real-Time Performance**: <200ms latency for surgical-grade responsiveness
* **Local-First Processing**: No cloud dependency for privacy and reliability
* **Adaptive Personalization**: Learns individual surgeon gesture patterns
* **Surgical Workflow Optimized**: Designed specifically for sterile OR environments
* **Lightweight AI**: Runs on standard hardware without GPU requirements

---


<table align="center">
<tr>
<td align="center"><img src="https://img.shields.io/badge/-Yeswanth%20Ram-6C5CE7?style=for-the-badge&logo=github&logoColor=white"></td>
<td align="center"><img src="https://img.shields.io/badge/-BharaniDharan%20-00CEC9?style=for-the-badge&logo=github&logoColor=white"></td>
<td align="center"><img src="https://img.shields.io/badge/-AsikKani%20-FDCB6E?style=for-the-badge&logo=github&logoColor=white"></td>
<td align="center"><img src="https://img.shields.io/badge/-Logesh%20-FF7675?style=for-the-badge&logo=github&logoColor=white"></td>
</tr>
</table>

---

## üë®‚Äçüíª Project Collaborators

<table align="center">
<tr>
<th>Role</th>
<th>Team Member</th>
<th>GitHub</th>
</tr>
<tr>
<td><b>üéØ Vision & Gaze Lead</b><br/><i>Gesture Tracking, Eye Tracking, Visual Feedback</i></td>
<td><b>Raghavan</b></td>
<td><a href="https://github.com/Raghavan7777"><img src="https://img.shields.io/badge/GitHub-Profile-181717?style=flat-square&logo=github"></a></td>
</tr>
<tr>
<td><b>üéôÔ∏è Audio & Intent Lead</b><br/><i>Voice Recognition, Command Parsing, AI Integration</i></td>
<td><b>Yeswanth Ram</b></td>
<td><a href="https://github.com/Yeswanthram28"><img src="https://img.shields.io/badge/GitHub-Profile-181717?style=flat-square&logo=github"></a></td>
</tr>
<tr>
<td><b>üß† Fusion & Integration Lead</b><br/><i>Multimodal Fusion, System Integration, Testing</i></td>
<td><b>VetriSelvan</b></td>
<td><a href="https://github.com/njr-vetri"><img src="https://img.shields.io/badge/GitHub-Profile-181717?style=flat-square&logo=github"></a></td>
</tr>
</table>

---

<div align="center">

<img src="https://img.shields.io/badge/Built%20With-‚ù§Ô∏è-red?style=for-the-badge">
<img src="https://img.shields.io/badge/Focus-Accessible%20Technology-blue?style=for-the-badge">

</div>

---

> *"Surgical technology should be intelligent, hands-free, and seamlessly integrated into the workflow."*

---
